---
output:
  pdf_document: default
  html_document: default
---
# Modeling continuous relationships

Most people are familiar with the concept of *correlation*, and in this chapter we will provide a more formal understanding for this commonly used and misunderstood concept.

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(fivethirtyeight)
library(BayesMed)

library(pander)
panderOptions('round',2)
panderOptions('digits',7)

set.seed(123456) # set random seed to exactly replicate results

# load the NHANES data library
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all=TRUE)

NHANES_adult <- 
  NHANES %>%
  drop_na(Weight) %>%
  subset(Age>=18)

```

## An example

In 2017, the web site Fivethirtyeight.com published a story titled [Higher Rates Of Hate Crimes Are Tied To Income Inequality]  (https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/) which discussed the relationship between the prevalence of hate crimes and income inequality in the wake of the 2016 Presidential election. The story reported an analysis of hate crime data from the FBI and the Southern Poverty Law Center, on the basis of which they report:

> "we found that income inequality was the most significant determinant of population-adjusted hate crimes and hate incidents across the United States".  

The data for this analysis are included in the ``fivethirtyeight`` R package, which makes it easy for us to access them.  The ``hate_crimes`` data frame includes data from each state, with two variables specific to hate crimes:

TBD: FIX FORMATTING

Header | Definition
---|---------
`state` | State name
`median_household_income` | Median household income, 2016
`share_unemployed_seasonal` | Share of the population that is unemployed (seasonally adjusted), Sept. 2016
`share_population_in_metro_areas` | Share of the population that lives in metropolitan areas, 2015
`share_population_with_high_school_degree` | Share of adults 25 and older with a high-school degree, 2009
`share_non_citizen` | Share of the population that are not U.S. citizens, 2015
`share_white_poverty` | Share of white residents who are living in poverty, 2015
`gini_index` | Gini Index, 2015
`share_non_white` | Share of the population that is not white, 2015
`share_voters_voted_trump` | Share of 2016 U.S. presidential voters who voted for Donald Trump
`hate_crimes_per_100k_splc` | Hate crimes per 100,000 population, Southern Poverty Law Center, Nov. 9-18, 2016
`avg_hatecrimes_per_100k_fbi` | Average annual hate crimes per 100,000 population, FBI, 2010-2015

The analysis reported in the story focused on the relationship between income inequality (defined by a quantity called the *Gini index*) and the prevalence of hate crimes in each state.  

### Quantifying inequality: The Gini index

Before we look at the analysis reported in the story, it's first useful to understand how the Gini index is used  to quantify inequality. The Gini index is usually defined in terms of a curve that describes the relation between income and the proportion of the population that has income at or less than that level, known as a *Lorenz curve*.  However, another way to think of it is more intuitive: It is the relative mean absolute difference between incomes, divided by two (from https://en.wikipedia.org/wiki/Gini_coefficient):

$$
G = \frac{\displaystyle{\sum_{i=1}^n \sum_{j=1}^n \left| x_i - x_j \right|}}{\displaystyle{2n\sum_{i=1}^n x_i}} 
$$

```{r echo=FALSE}
# function to generate a plot of Lorenz curve and compute Gini coefficient
lorenzCurve = function(df){
  df <- df %>% arrange(income)
  sumIncome <- sum(df$income)
  lc <- array(NA,nrow(df)+1)
  p <- array(NA,nrow(df)+1)
  lc[1] <- 0
  p[1] <- 0
  for (i in 1:nrow(df)){
    lc[i+1] <- sum(df$income[1:i])/sumIncome
    p[i+1] <- i/nrow(df)
  }
  S <- sum(lc)
  giniCoef <- 1 + (1-2*S)/nrow(df)

  p <- ggplot(data.frame(p,lc),aes(p,lc)) + 
    geom_line(color='blue') + 
    geom_point() + 
    xlim(0,1) + ylim(0,1) + 
    xlab('proportion of population') + ylab('proportion of income') +
    geom_abline(slope=1,intercept = 0,color='black',linetype='dotted') +
    ggtitle(sprintf('Gini coefficient = %f',giniCoef))
  print(p)
  return(giniCoef)
}

```

First, let's create an example with 10 people where everyone has exactly the same income (Figure \@ref(fig:gini0)).

```{r gini0,fig.cap="Lorenz curve for Gini index of zero (i.e. perfect equality)."}

incomeDf <- data.frame(income=rep(40000,10))
print(sprintf('Gini index = %0.2f',lorenzCurve(incomeDf)))
```
Now let's look at an example where income is normally distributed (Figure \@ref(fig:giniNormal)).

```{r giniNormal,fig.cap="Gini index for a population with normally distributed income."}
incomeDf <- data.frame(income=rnorm(10,mean=40000,sd=5000))

print(sprintf('Gini index = %0.2f',lorenzCurve(incomeDf)))

```

Now let's look at an example with high inequality; everyone has equal income (\$40,000) except for one person, who has income of \$40,000,000 (Figure \@ref(fig:giniInequal)).

```{r giniInequal,fig.cap="Lorenz curve for a maximally inequal population."}
incomeDf <- data.frame(income=rnorm(10,mean=40000,sd=5000))
incomeDf$income[1] <- 40000000

print(sprintf('Gini index = %0.2f',lorenzCurve(incomeDf)))

```

According to the US Census, the United States had a Gini index of 0.469 in 2010, falling roughly half way between our normally distributed and maximally inequal examples.

## Is income inequality related to hate crimes?

Now that we understand the Gini index, we can look at the relationship between income inequality and rates of hate crimes (see Figure \@ref(fig:hateCrimeGini)).

```{r hateCrimeGini, fig.cap="Plot of rates of hate crimes vs. Gini index."}

hateCrimes <- 
  hate_crimes %>%
  mutate(state_abb = state.abb[match(state,state.name)]) %>%
  drop_na(avg_hatecrimes_per_100k_fbi)

hateCrimes$state_abb[hateCrimes$state=="District of Columbia"]='DC'

ggplot(hateCrimes,aes(gini_index,avg_hatecrimes_per_100k_fbi,label=state_abb)) +
  geom_point() + geom_text(aes(label=state_abb),hjust=0, vjust=0) +
  theme(plot.title = element_text(size = 20, face = "bold")) +
  xlab('Gini index') + 
  ylab('Avg hate crimes per 100K population (FBI)')
  

```

Looking at the data, it seems that there may be a positive relationship between the two variables.  How can we quantify that relationship?

## Covariance and correlation

One way to quantify the relationship between two variables is the *covariance*.  Remember that variance for a single variable is computed as:

$$
s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{N - 1}
$$

This tells us how far each observation is from the mean.  Covariance tells us whether there is a relation between the deviations of two different variables across observations.  It is defined as:

$$
covariance = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{N - 1}
$$

This value will be high when x and y are both highly deviant from the mean; if they are deviant in the same direction then the covariance is positive, whereas if they are deviant in opposite directions the covariance is negative.  Let's look at a toy example first.

```{r }
df <- 
  data.frame(x=c(3,5,8,10,12)) %>%
  mutate(y=x+round(rnorm(5,sd=2))) %>%
  mutate(
    y_dev=y-mean(y),
    x_dev=x-mean(x)
  ) %>%
  mutate(crossproduct=y_dev*x_dev)

pander(df)
pander(sprintf('sum of cross products = %.2f\n',sum(df$crossproduct)))
covXY <- sum(df$crossproduct)/(nrow(df)-1)
pander(sprintf('covariance: %.2f\n',covXY))
# compare to cov(df$x,df$y)

```

We don't usually use the covariance to describe relationships between variables, because it varies with the overall lever of variance in the data.  Instead, we would usually use the *correlation coefficient* (often referred to as *Pearson's correlation* after the statistician Karl Pearson). The correlation is computed by scaling the covariance by the standard deviations of the two variables:

$$
r = \frac{covariance}{s_xs_y} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(N - 1)s_x s_y}
$$

```{r}
corXY <- sum(df$crossproduct)/((nrow(df)-1)*sd(df$x)*sd(df$y))
pander(sprintf('correlation coefficient = %.2f\n',corXY))
# compare to cor(df$x,df$y)
```
 
The correlation coefficient is useful because it varies between -1 and 1 regardless of the nature of the data - in fact, we already discussed the correlation coefficient earlier in the discussion of effect sizes.  As we saw in the previous chapter on effect sizes, a correlation of 1 indicates a perfect linear relationship, a correlation of -1 indicates a perfect negative relationship, and a correlation of zero indicates no linear relationship.

We can compute the correlation coefficient for the hate crime data:

```{r}
corGiniHC <- cor(hateCrimes$gini_index,
                 hateCrimes$avg_hatecrimes_per_100k_fbi)
pander(sprintf('correlation coefficient = %.2f\n',corGiniHC))
```

### Hypothesis testing for correlations

A correlation value of `r I(corGiniHC)` seems to indicate a reasonably strong relationship between the two variables, but we can also imagine that this could occur by chance even if there is no relationship.  We can test the null hypothesis that the correlation is zero, using a simple equation that lets us convert a correlation value into a *t* statistic:

$$
\textit{t}_r =  \frac{r\sqrt{N-2}}{\sqrt{1-r^2}}
$$

Under the null hypothesis $H_0:r=0$, this statistic is distributed as a t distribution with $N - 2$ degrees of freedom.  We can compute this using the ``cor.test()`` function in R:

```{r}
cor.test(hateCrimes$avg_hatecrimes_per_100k_fbi,
         hateCrimes$gini_index)
```

This test shows that the likelihood of an r value this extreme or more is quite low, so we would reject the null hypothesis of $r=0$.  Note that this test assumes that both variables are normally distributed.

We could also test this by randomization, in which we repeatedly shuffle the values of one of the variables and compute the correlation, and then compare our observed correlation value to this null distribution to determine how likely our observed value would be under the null hypothesis.

```{r shuffleCorr,fig.cap="Histogram of correlation values under the null hypothesis, obtained by shuffling values. Observed value is denoted by blue line."}
shuffleCorr <- function(x,y){
  xShuffled=sample(x)
  return(cor(xShuffled,y))
}

shuffleDist <- replicate(2500,shuffleCorr(hateCrimes$avg_hatecrimes_per_100k_fbi,hateCrimes$gini_index))

ggplot(data.frame(shuffleDist),aes(shuffleDist)) + 
  geom_histogram(bins=100) +
  geom_vline(xintercept = corGiniHC,color='blue') +
  ggtitle(sprintf('p(shuffled r >= observed) = %0.3f',mean(shuffleDist>=corGiniHC))) +
  theme(plot.title = element_text(size = 20, face = "bold"))

```

Randomization gives us a reasonably similar answer to the t-test.

### Robust correlations

You may have noticed something a bit odd in Figure \@ref(fig:hateCrimeGini) -- one of the datapoints seemed to be quite separate from the others.  We refer to this as an *outlier*, and the standard correlation coefficient is very sensitive to outliers.  For example, in Figure \@ref(fig:outlierCorr) we can see how a single outlying data point can cause a very high positive correlation value, even when the actual relationship between the other data points is perfectly negative.

```{r outlierCorr, fig.cap="An example of the effects of outliers on correlation.  Without the outlier the remainder of the datapoints have a perfect negative correlation, but the single outlier changes the correlation value to highly positive."}
n <- 10
set.seed(1234)

dfOutlier <- 
  data.frame(x=rnorm(n)) %>%
  mutate(y=x*-1)

dfOutlier$x[1] <- 10
dfOutlier$y[1] <- 10
cc <- cor(dfOutlier$x,dfOutlier$y)
ccSpearman <- cor(dfOutlier$x,dfOutlier$y,method='spearman')

p <- ggplot(dfOutlier,aes(x,y))+
   geom_point() +
   ggtitle(sprintf('r = %0.2f (without outlier: r = %.2f)',cc,cor(dfOutlier$x[2:n],dfOutlier$y[2:n]))) +
   theme(plot.title = element_text(size = 20, face = "bold"))
 print(p)


```

One way to address outliers is to compute the correlation on the ranks of the data after ordering them, rather than on the data themselves; this is known as *Spearman's correlation*.  Whereas the Pearson correlation for the example in Figure \@ref(fig:outlierCorr) was r `I(cc)`, the Spearman correlation is `r I(ccSpearman)`, showing that the rank correlation reduces the effect of the outlier.

We can compute the rank correlation on the hate crime data using the `cor.test` function:

```{r}
corTestSpearman <- cor.test(dfOutlier$x,dfOutlier$y,method='spearman')
corTestSpearman
```

Now we see that the correlation is no longer significant (and in fact the estimate is negative!), suggesting that the claims of the FiveThirtyEight blog post may have been incorrect due to the effect of the outlier.

### Bayesian correlation analysis

We can also analyze the FiveThirtyEight data using Bayesian analysis, which has two advantages.  First, it provides us with a posterior probability -- in this case, the probability that the correlation value exceeds zero.  Second, the Bayesian estimate combines the observed evidence with a *prior*, which has the effect of *regularizing* the correlation estimate, effectively pulling it towards zero.  Here we can compute it using the `jzs_cor` function from the BayesMed package.


```{r}
bayesCor <- jzs_cor(hateCrimes$avg_hatecrimes_per_100k_fbi,
        hateCrimes$gini_index)
bayesCor

```

Notice that the correlation estimated using the Bayesian method is slightly smaller than the one estimated using the standard correlation coefficient, which is due to the fact that the estimate is based on a combination of the evidence and the prior, which effectively shrinks the estimate toward zero. However, notice that the Bayesian analysis is not robust to the outlier, and it still says that there is fairly strong evidence that the correlation is greater than zero.

## Correlation and causation

When we say that one thing *causes* another, what do we mean?  There is a long history in philosophy of discussion about the meaning of causality, but in statistics one way that we commonly think of causation is in terms of experimental control.  That is, if we think that factor X causes factor Y, then manipulating the value of X should also manipulate the value of Y.

In medicine, there is a set of ideas known as [*Koch's postulates*](https://en.wikipedia.org/wiki/Koch%27s_postulates) which have historically been used to determine whether a particular organism causes a disease.   The basic idea is that the organism should be present in people with the disease, and not present in those without it -- thus, a treatment that eliminates the organism should also eliminate the disease.  Further, infecting someone with the organism should cause them to contract the disease.  An example of this was seen in the work of Dr. Barry Marshall, who had a hypothesis that stomach ulcers were caused by a bacterium (*Helicobacter pylori*).  To demonstrate this, he infected himself with the bacterium, and soon thereafter developed severe inflammation in his stomach.  He then treated himself with an antibiotic, and his stomach soon recovered.  He later won the Nobel Prize in Medicine for this work.

Often we would like to test causal hypotheses but we can't actually do an experiment, either because it's impossible ("What is the relationship between human carbon emissions and the earth's climate?") or unethical ("What are the effects of severe abuse on child brain development?"). However, we can still collect data that might be relevant to those questions.  For example, in the latter example, we can potentially collect data from children who have been abused as well as those who have not, and we can then ask whether their brain development differs.

Let's say that we did such an analysis, and we found that abused children had poorer brain development than non-abused children. Would this demonstrate that abuse *causes* poorer brain development?  No.  Whenever we observe a statistical association between two variables, it is certainly possible that one of those two variables causes the other.  However, it is also possible that both of the variables are being influenced by a third variable; in this example, it could be that child abuse is associated with family stress, which could also cause poorer brain development through less intellectual engagement, food stress, or many other possible avenues.  The point is that a correlation between two variables generally tells us that something is causing somethign else, but it doesn't tell us what is causing what.  As the statistician Edward Tufte says, "Correlation does not imply causation, but it's a pretty good hint."

### Causal graphs

One useful way to describe causal relations between variables is through a *causal graph*, which shows variables as circles and causal relations between them as arrows.  For example, Figure \@ref(fig:simpleCausalGraph) shows the causal relationships between study time and two variables that we think should be affected by it: exam grades and exam finishing times.  

```{r simpleCausalGraph, fig.cap="A graph showing causal relationships between three variables: study time, exam grades, and exam finishing time.  A green arrow represents a positive relationship (i.e. more study time causes exam grades to increase), and a red arrow represents a negative relationship (i.e. more study time causes faster completion of the exam)."}

library(DiagrammeR)
grViz("
digraph boxes_and_circles {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
 
  node [shape = circle,
        fixedsize = true,
        width = 0.9,
        fontsize=10] // sets as circles
  StudyTime; ExamGrade; FinishTime

  # several 'edge' statements
  StudyTime->ExamGrade [color=green]
  StudyTime->FinishTime [color=red]
}
")
```

However, in reality the effects on finishing time and grades are not due directly to the amount of time spent studying, but rather to the amount of knowledge that the student gains by studying.  We would usually say that knowledge is a *latent* variable -- that is, we can't measure it directly but we can see it reflected in variables that we can measure (like grades and finishing times).  Figure \@ref(fig:latentCausalGraph) shows this.

```{r latentCausalGraph, fig.cap="A graph showing the same causal relationships as above, but now also showing the latent variable (knowledge) using a square box."}

library(DiagrammeR)
grViz("
digraph boxes_and_circles {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
 
  node [shape = circle,
        fixedsize = true,
        width = 0.9] // sets as circles
  StudyTime; ExamGrade; FinishTime; Knowledge

  # several 'edge' statements
  StudyTime->Knowledge [color=green]
  Knowledge->ExamGrade [color=green]
  Knowledge->FinishTime [color=red]
}
")
```

Here we would say that knowledge *mediates* the relationship between study time and grades/finishing times.  That means that if we were able to hold knowledge constant (for example, by administering a drug that causes immediate forgetting), then the amount of study time should no longer have an effect on grades and finishing times.

Note that if we simply measured exam grades and finishing times we would generally see negative relationship between them, because people who finish exams the fastest in general get the highest grades.  However, if we were to interpret this correlation as a causal relation, this would tell us that in order to get better grades, we should actually finish the exam more quickly! This example shows how tricky the inference of causality from non-experimental data can be.

Within statistics and machine learning, there is a very active research community that is currently studying the question of when and how we can infer causal relationships from non-experimental data.  If you would like an introduction to this area, I would recommend [The Book of Why](http://bayes.cs.ucla.edu/WHY/) by Judea Pearl and Dana Mackenzie.

## Suggested readings

